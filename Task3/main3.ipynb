{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55e720c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7862\n",
      "* Running on public URL: https://b3642216aa95a9bba7.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://b3642216aa95a9bba7.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import gradio as gr\n",
    "import pandas as pd\n",
    "\n",
    "# Residual block for feature learning\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(channels)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "# Complex U-Net with skip connections and residual blocks\n",
    "class ComplexColorizationNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Encoder\n",
    "        self.enc1 = nn.Sequential(\n",
    "            nn.Conv2d(4, 64, 3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            ResidualBlock(64)\n",
    "        )\n",
    "        self.enc2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, 3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            ResidualBlock(128)\n",
    "        )\n",
    "        self.enc3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, 3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            ResidualBlock(256)\n",
    "        )\n",
    "        self.enc4 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, 3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            ResidualBlock(512)\n",
    "        )\n",
    "        # Bottleneck\n",
    "        self.bottleneck = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, 3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            ResidualBlock(512)\n",
    "        )\n",
    "        # Decoder\n",
    "        self.up4 = nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1)\n",
    "        self.dec4 = nn.Sequential(\n",
    "            nn.BatchNorm2d(512),  # 256 + 256 channels after concat\n",
    "            nn.ReLU(inplace=True),\n",
    "            ResidualBlock(512)\n",
    "        )\n",
    "        self.up3 = nn.ConvTranspose2d(512, 128, kernel_size=4, stride=2, padding=1)\n",
    "        self.dec3 = nn.Sequential(\n",
    "            nn.BatchNorm2d(256),  # 128 + 128\n",
    "            nn.ReLU(inplace=True),\n",
    "            ResidualBlock(256)\n",
    "        )\n",
    "        self.up2 = nn.ConvTranspose2d(256, 64, kernel_size=4, stride=2, padding=1)\n",
    "        self.dec2 = nn.Sequential(\n",
    "            nn.BatchNorm2d(128),  # 64 + 64\n",
    "            nn.ReLU(inplace=True),\n",
    "            ResidualBlock(128)\n",
    "        )\n",
    "        self.dec1 = nn.Sequential(\n",
    "            nn.Conv2d(128, 32, 3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.out_conv = nn.Conv2d(32, 2, kernel_size=1)  # ab channels output\n",
    "\n",
    "    def forward(self, x):\n",
    "        e1 = self.enc1(x)      # (B,64,H,W)\n",
    "        e2 = self.enc2(e1)     # (B,128,H/2,W/2)\n",
    "        e3 = self.enc3(e2)     # (B,256,H/4,W/4)\n",
    "        e4 = self.enc4(e3)     # (B,512,H/8,W/8)\n",
    "\n",
    "        b = self.bottleneck(e4)  # (B,512,H/8,W/8)\n",
    "\n",
    "        d4 = self.up4(b)          # (B,256,H/4,W/4)\n",
    "        d4 = torch.cat([d4, e3], dim=1)  # (B,512,H/4,W/4)\n",
    "        d4 = self.dec4(d4)        # (B,512,H/4,W/4)\n",
    "\n",
    "        d3 = self.up3(d4)         # (B,128,H/2,W/2)\n",
    "        d3 = torch.cat([d3, e2], dim=1)  # (B,256,H/2,W/2)\n",
    "        d3 = self.dec3(d3)        # (B,256,H/2,W/2)\n",
    "\n",
    "        d2 = self.up2(d3)         # (B,64,H,W)\n",
    "        d2 = torch.cat([d2, e1], dim=1)  # (B,128,H,W)\n",
    "        d2 = self.dec2(d2)        # (B,128,H,W)\n",
    "\n",
    "        d1 = self.dec1(d2)        # (B,32,H,W)\n",
    "        out = torch.tanh(self.out_conv(d1))  # (B,2,H,W)\n",
    "        return out\n",
    "\n",
    "model = ComplexColorizationNet()\n",
    "model.eval()\n",
    "\n",
    "def preprocess_image(image):\n",
    "    image = image.convert('RGB').resize((256, 256))\n",
    "    gray = cv2.cvtColor(np.array(image), cv2.COLOR_RGB2GRAY)\n",
    "    gray_norm = gray.astype(np.float32) / 255.0\n",
    "    return gray_norm, image\n",
    "\n",
    "def prepare_input_tensor(gray, hint_points):\n",
    "    h, w = gray.shape\n",
    "    L = torch.from_numpy(gray).unsqueeze(0).unsqueeze(0)  # (1,1,H,W)\n",
    "    ab_hint = torch.zeros(1, 2, h, w)\n",
    "    mask = torch.zeros(1, 1, h, w)\n",
    "    for (x, y, ab) in hint_points:\n",
    "        x = min(max(0, x), w-1)\n",
    "        y = min(max(0, y), h-1)\n",
    "        ab_hint[0, :, y, x] = torch.tensor(ab)\n",
    "        mask[0, 0, y, x] = 1\n",
    "    input_tensor = torch.cat([L, ab_hint, mask], dim=1)  # (1,4,H,W)\n",
    "    return input_tensor\n",
    "\n",
    "def postprocess_output(L, ab):\n",
    "    ab_np = ab.detach().cpu().numpy().squeeze()\n",
    "    h, w = L.shape\n",
    "    ab_resized = np.zeros((2, h, w), dtype=np.float32)\n",
    "    for i in range(2):\n",
    "        ab_resized[i] = cv2.resize(ab_np[i], (w, h))\n",
    "    L_np = (L * 100).astype(np.float32)\n",
    "    lab = np.zeros((h, w, 3), dtype=np.float32)\n",
    "    lab[:, :, 0] = L_np\n",
    "    lab[:, :, 1:] = ab_resized.transpose(1, 2, 0) * 110\n",
    "    lab = lab.astype(np.float32)\n",
    "    rgb = cv2.cvtColor(lab, cv2.COLOR_LAB2RGB)\n",
    "    rgb = np.clip(rgb, 0, 1)\n",
    "    rgb_img = (rgb * 255).astype(np.uint8)\n",
    "    return Image.fromarray(rgb_img)\n",
    "\n",
    "user_hints = []\n",
    "\n",
    "def clear_hints():\n",
    "    global user_hints\n",
    "    user_hints = []\n",
    "\n",
    "def add_user_hint(x, y, r, g, b):\n",
    "    global user_hints\n",
    "    rgb = np.array([[[r / 255.0, g / 255.0, b / 255.0]]], dtype=np.float32)\n",
    "    lab = cv2.cvtColor(rgb, cv2.COLOR_RGB2LAB)[0, 0]\n",
    "    a = (lab[1] - 128) / 128\n",
    "    b_ = (lab[2] - 128) / 128\n",
    "    user_hints.append((int(x), int(y), (a, b_)))\n",
    "\n",
    "def model_colorize(image, hint_points_df):\n",
    "    gray_norm, _ = preprocess_image(image)\n",
    "    clear_hints()\n",
    "    if hint_points_df is not None and not hint_points_df.empty:\n",
    "        for row in hint_points_df.values.tolist():\n",
    "            if len(row) != 3:\n",
    "                continue\n",
    "            x, y, color_hex = row\n",
    "            try:\n",
    "                x, y = int(x), int(y)\n",
    "                c = color_hex.strip()\n",
    "                if not c.startswith(\"#\"):\n",
    "                    c = \"#\" + c\n",
    "                r, g, b = tuple(int(c.lstrip('#')[i:i + 2], 16) for i in (0, 2, 4))\n",
    "                add_user_hint(x, y, r, g, b)\n",
    "            except Exception:\n",
    "                continue\n",
    "    input_tensor = prepare_input_tensor(gray_norm, user_hints)\n",
    "    with torch.no_grad():\n",
    "        output_ab = model(input_tensor)\n",
    "    return postprocess_output(gray_norm, output_ab)\n",
    "\n",
    "def get_image_metrics(image):\n",
    "    gray = image.convert(\"L\")\n",
    "    np_gray = np.array(gray)\n",
    "    return {\n",
    "        \"Resolution\": f\"{image.width}x{image.height}\",\n",
    "        \"Unique Gray Levels\": np.unique(np_gray).size,\n",
    "        \"Hint Points Count\": len(user_hints)\n",
    "    }\n",
    "\n",
    "def add_hint_row(hint_points):\n",
    "    if hint_points is None:\n",
    "        hint_points = pd.DataFrame(columns=[\"x\", \"y\", \"color\"])\n",
    "    new_row = pd.DataFrame([[0, 0, \"#ff0000\"]], columns=hint_points.columns)\n",
    "    return pd.concat([hint_points, new_row], ignore_index=True)\n",
    "\n",
    "def reset_hints():\n",
    "    return pd.DataFrame(columns=[\"x\", \"y\", \"color\"])\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"## Complex Interactive User-Guided Image Colorization\")\n",
    "\n",
    "    with gr.Row():\n",
    "        img_in = gr.Image(label=\"Upload grayscale image\", type=\"pil\")\n",
    "        color_out = gr.Image(label=\"Colorized Output\")\n",
    "\n",
    "    with gr.Row():\n",
    "        point_input = gr.Dataframe(\n",
    "            headers=[\"x\", \"y\", \"color\"],\n",
    "            datatype=[\"number\", \"number\", \"str\"],\n",
    "            label=\"Hint Locations and Colors\",\n",
    "            max_height=250,\n",
    "            interactive=True\n",
    "        )\n",
    "        add_hint_btn = gr.Button(\"Add Hint\")\n",
    "        reset_btn = gr.Button(\"Reset Hints\")\n",
    "\n",
    "    add_hint_btn.click(add_hint_row, inputs=[point_input], outputs=[point_input])\n",
    "    reset_btn.click(reset_hints, outputs=[point_input])\n",
    "\n",
    "    run_btn = gr.Button(\"Colorize\")\n",
    "    resolution_lbl = gr.Label(value=\"Resolution: N/A\")\n",
    "    unique_gray_lbl = gr.Label(value=\"Unique Gray Levels: N/A\")\n",
    "    hint_count_lbl = gr.Label(value=\"Hint Points Count: 0\")\n",
    "\n",
    "    def run_colorization(image, points_df):\n",
    "        out_img = model_colorize(image, points_df)\n",
    "        metrics = get_image_metrics(image)\n",
    "        return out_img, metrics[\"Resolution\"], metrics[\"Unique Gray Levels\"], metrics[\"Hint Points Count\"]\n",
    "\n",
    "    run_btn.click(\n",
    "        fn=run_colorization,\n",
    "        inputs=[img_in, point_input],\n",
    "        outputs=[color_out, resolution_lbl, unique_gray_lbl, hint_count_lbl]\n",
    "    )\n",
    "\n",
    "demo.launch(share=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce088cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
